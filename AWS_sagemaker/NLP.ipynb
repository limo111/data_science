{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Sentiment Analysis for Movie Reviews using SageMaker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "Obtain a dataset of labeled movie reviews, where each review is labeled as positive or negative sentiment.\n",
    "Preprocess the text data by removing punctuation, lowercasing the text, and performing tokenization.\n",
    "Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "\n",
    "# Load the dataset of labeled movie reviews (assuming it's in a CSV format)\n",
    "data = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "# Preprocessing: Remove punctuation, lowercase the text, and perform tokenization\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization (split the text into individual words)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data['preprocessed_text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)\n",
    "print(\"Testing labels shape:\", test_labels.shape)\n",
    "\n",
    "# Save the preprocessed data and labels as separate CSV files\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "train_labels.to_csv('train_labels.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "test_labels.to_csv('test_labels.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "Create an Amazon S3 bucket to store the preprocessed data and model artifacts.\n",
    "Use SageMaker's built-in algorithms or custom-built models for training.\n",
    "Choose an algorithm such as linear learners, convolutional neural networks (CNN), or recurrent neural networks (RNN).\n",
    "Configure hyperparameters like learning rate, batch size, and number of epochs.\n",
    "Train the model using the labeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "# Set up the SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Set up the S3 bucket to store data and model artifacts\n",
    "bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'sagemaker/sentiment-analysis'  # Prefix for the data and model artifacts in the bucket\n",
    "\n",
    "# Upload the preprocessed data to the S3 bucket\n",
    "train_data_s3 = sagemaker_session.upload_data(path='train_data.csv', bucket=bucket, key_prefix=prefix+'/train')\n",
    "train_labels_s3 = sagemaker_session.upload_data(path='train_labels.csv', bucket=bucket, key_prefix=prefix+'/train')\n",
    "test_data_s3 = sagemaker_session.upload_data(path='test_data.csv', bucket=bucket, key_prefix=prefix+'/test')\n",
    "test_labels_s3 = sagemaker_session.upload_data(path='test_labels.csv', bucket=bucket, key_prefix=prefix+'/test')\n",
    "\n",
    "# Configure and train the model using SageMaker's built-in algorithms or custom-built models\n",
    "\n",
    "# Example using LinearLearner (built-in algorithm)\n",
    "container = get_image_uri(sagemaker_session.boto_region_name, 'linear-learner')\n",
    "\n",
    "linear_learner = sagemaker.estimator.Estimator(container,\n",
    "                                               role,\n",
    "                                               train_instance_count=1,\n",
    "                                               train_instance_type='ml.c4.xlarge',\n",
    "                                               output_path=f's3://{bucket}/{prefix}/output',\n",
    "                                               sagemaker_session=sagemaker_session)\n",
    "\n",
    "linear_learner.set_hyperparameters(feature_dim=100,  # Replace with the appropriate feature dimension\n",
    "                                   predictor_type='binary_classifier',  # Assuming binary sentiment classification\n",
    "                                   epochs=10,\n",
    "                                   mini_batch_size=32)\n",
    "\n",
    "train_input = s3_input(s3_data=train_data_s3, content_type='text/csv')\n",
    "validation_input = s3_input(s3_data=test_data_s3, content_type='text/csv')\n",
    "\n",
    "linear_learner.fit({'train': train_input, 'validation': validation_input})\n",
    "# Specify the S3 location to store the model artifacts\n",
    "model_path = f's3://{bucket}/{prefix}/model'\n",
    "\n",
    "# Save the model artifacts to S3\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The code starts by importing the necessary modules from the SageMaker library.\n",
    "The SageMaker session and execution role are set up using sagemaker.Session() and get_execution_role() respectively.\n",
    "You need to specify your S3 bucket name in the bucket variable and a prefix for the data and model artifacts in the prefix variable.\n",
    "The preprocessed data (train and test) and labels are uploaded to the S3 bucket using sagemaker_session.upload_data().\n",
    "The example includes two scenarios: one using the built-in LinearLearner algorithm and another using a custom-built model. You can choose the approach that fits your requirements.\n",
    "For the built-in LinearLearner algorithm, the code sets up the estimator object with the necessary configurations such as the container, role, instance type, and hyperparameters. It then fits the estimator to the training and validation data.\n",
    "For a custom-built model, you would need to define and configure your model accordingly, and then train it using your preferred deep learning framework (e.g., TensorFlow or PyTorch)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Evaluation\n",
    "\n",
    "Evaluate the trained model using the labeled testing data.\n",
    "Calculate metrics such as accuracy, precision, recall, and F1 score to assess the model's performance.\n",
    "Fine-tune the model if necessary based on the evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Set up the SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Set up the S3 bucket and prefix\n",
    "bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'sagemaker/sentiment-analysis'  # Prefix for the data and model artifacts in the bucket\n",
    "\n",
    "# Load the trained model\n",
    "model = sagemaker.LinearLearnerModel(model_data=f's3://{bucket}/{prefix}/output/model.tar.gz',\n",
    "                                     role=role,\n",
    "                                     sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Create a predictor object for model evaluation\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
    "\n",
    "# Set up the input data for evaluation\n",
    "test_data = sagemaker_session.upload_data(path='test_data.csv', bucket=bucket, key_prefix=prefix + '/test')\n",
    "test_labels = sagemaker_session.upload_data(path='test_labels.csv', bucket=bucket, key_prefix=prefix + '/test')\n",
    "\n",
    "# Configure the predictor to accept CSV input\n",
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "\n",
    "# Perform model evaluation\n",
    "results = predictor.predict(test_data)\n",
    "\n",
    "# Convert the predicted results to a list of labels\n",
    "predicted_labels = [round(float(result)) for result in results]\n",
    "\n",
    "# Load the actual labels from the CSV file\n",
    "actual_labels = pd.read_csv('test_labels.csv')\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = sum(predicted_labels == actual_labels) / len(actual_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Delete the predictor endpoint\n",
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The code begins by importing the necessary modules from the SageMaker library.\n",
    "The SageMaker session and execution role are set up using sagemaker.Session() and get_execution_role() respectively.\n",
    "You need to specify your S3 bucket name in the bucket variable and a prefix for the data and model artifacts in the prefix variable.\n",
    "The trained model is loaded using sagemaker.LinearLearnerModel(), specifying the S3 location of the model artifacts.\n",
    "A predictor object is created by deploying the model to a SageMaker endpoint using model.deploy().\n",
    "The test data and labels are uploaded to the S3 bucket using sagemaker_session.upload_data().\n",
    "The predictor is configured to accept CSV input and perform predictions on the test data using predictor.predict().\n",
    "The predicted results are converted to a list of labels, and the actual labels are loaded from the CSV file.\n",
    "Evaluation metrics such as accuracy, precision, recall, and F1 score are calculated using appropriate functions (e.g., precision_score(), recall_score(), f1_score()).\n",
    "The evaluation metrics are printed, and the predictor endpoint is deleted using sagemaker.Session().delete_endpoint()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Model deployment:\n",
    "\n",
    "Deploying the trained model as an endpoint using SageMaker.\n",
    "Configuring the endpoint to handle incoming text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Set up the SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Set up the S3 bucket and prefix\n",
    "bucket = 'your-s3-bucket-name'  # Replace with your S3 bucket name\n",
    "prefix = 'sagemaker/sentiment-analysis'  # Prefix for the data and model artifacts in the bucket\n",
    "\n",
    "# Load the trained model\n",
    "model = sagemaker.LinearLearnerModel(model_data=f's3://{bucket}/{prefix}/output/model.tar.gz',\n",
    "                                     role=role,\n",
    "                                     sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Deploy the model as an endpoint\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
    "\n",
    "# Configure the predictor to handle incoming text data\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "# Example usage of the endpoint\n",
    "input_text = 'This is a great movie!'\n",
    "response = predictor.predict(input_text)\n",
    "print(response)\n",
    "\n",
    "# Delete the predictor endpoint\n",
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Explanation:\n",
    "\n",
    "The code begins by importing the necessary modules from the SageMaker library.\n",
    "The SageMaker session and execution role are set up using sagemaker.Session() and get_execution_role() respectively.\n",
    "You need to specify your S3 bucket name in the bucket variable and a prefix for the data and model artifacts in the prefix variable.\n",
    "The trained model is loaded using sagemaker.LinearLearnerModel(), specifying the S3 location of the model artifacts.\n",
    "The model is deployed as an endpoint using model.deploy(), specifying the number of instances and instance type for the endpoint.\n",
    "The predictor object is created for making predictions using the deployed endpoint.\n",
    "The predictor is configured to handle incoming text data by setting the serializer to CSVSerializer() to accept CSV formatted input and the deserializer to JSONDeserializer() to return predictions in JSON format.\n",
    "An example usage of the endpoint is shown, where input_text contains the text to be analyzed, and the response from the predictor is printed.\n",
    "Finally, the endpoint is deleted using sagemaker_session.delete_endpoint() to avoid incurring unnecessary costs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "\n",
    "AWS SageMaker documentation: Provides detailed instructions and code examples for training and deploying machine learning models using SageMaker.\n",
    "\n",
    "Link: https://docs.aws.amazon.com/sagemaker/\n",
    "AWS Developer Guide: Covers various aspects of machine learning on AWS, including data preparation, model training, and deployment.\n",
    "\n",
    "Link: https://aws.amazon.com/developers/guides/\n",
    "AWS Samples GitHub repository: Contains sample code and notebooks for different AWS services, including SageMaker.\n",
    "\n",
    "Link: https://github.com/aws-samples\n",
    "Amazon SageMaker Examples: Provides a collection of Jupyter notebooks with step-by-step guides for various machine learning tasks, including NLP.\n",
    "\n",
    "Link: https://github.com/aws/amazon-sagemaker-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
